---
title: "Simple Linear Regression"
subtitle: "ENVX1002 - Week 10"
date: today
date-format: "MMM YYYY"
author: 
  - name: Januar Harianto
    affiliation: School of Life and Envoronmental Sciences
institute: The University of Sydney
format:
  revealjs: 
    theme: [default, theme.scss]
    slide-number: c/t
    code-copy: true
    code-link: false
    code-overflow: wrap
    highlight-style: arrow
    html-math-method: katex
    embed-resources: false
execute: 
  eval: true
  echo: true
  freeze: auto  # re-render only when source changes
editor_options: 
  chunk_output_type: console
  render-on-save: true  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
  message = FALSE, 
  warning = FALSE,
  cache = TRUE)

if (!require("pacman")) install.packages("pacman", repos = "http://cran.us.r-project.org")
pacman::p_load(tidyverse, cowplot, HistData, datasauRus, patchwork, broom, remotes)
pacman::p_load_gh("datalorax/equatiomatic")

ggplot2::theme_set(cowplot::theme_half_open())
```

# Recap

## Last week...

- Correlation $r$: a measure of the strength and direction of the linear relationship between two variables
- Is there a causal relationship between two variables?
  - **No**: use correlation analysis
  - **Yes**: use *regression analysis*

. . .


### Simple linear regression modelling

$$ Y_i = \beta_0 + \beta_1 x_i + \epsilon_i $$ 


> Basically, a deterministic straight line equation $y=c+mx$, with added random variation that is normally distributed

$$ Y = c + mx + \epsilon $$


## Fitting the line 
$$ Y_i = \beta_0 + \beta_1 x_i + \epsilon_i $$ 

$$ Y = c + mx + \epsilon $$

How do we fit a line to data if data are "noisy"?

```{r}
#| code-fold: true
x <- 1:10
y <- 2*x + rnorm(10, 0, 2)
# generate y with predicted values
y_pred <- 2*x
df <- data.frame(x, y)

p1 <- ggplot(df, aes(x, y_pred)) +
  geom_point(size = 2) +
  geom_smooth(method = "lm", se = FALSE, color = "firebrick") +
  labs(x = "x", y = "y", title = "A")

p2 <- ggplot(df, aes(x, y)) +
  geom_point(size = 2) +
  geom_smooth(method = "lm", se = FALSE, color = "royalblue") +
  labs(x = "x", y = "y", title = "B (How do we fit this?)")

library(patchwork)
p1 + p2 + plot_layout(ncol = 2)
```

# Least squares

> The method of least squares is the **automobile of modern statistical analysis**: despite its limitations, ocassional accidents and incidental pollution, it and its numerous variations, extensions, and related conveyances **carry the bulk of statistical analyses**, and are known and valued by nearly all.

-- Stigler, 1981 (emphasis added)

## Usage

- **Student's t-test**
- **linear regression**
  
. . .

- ANOVA
- logistic regression
- nonlinear regression
- ridge regression
- lasso regression
- principle component analysis
- generalised linear model
- etc...

## History

January 1, 1801: Italian astronomer Giuseppe Piazzi discovers Ceres, the first and largest asteroid/dwarf planet.
  
![Source: [Wikipedia](https://en.wikipedia.org/wiki/Ceres_%28dwarf_planet%29)](assets/ceres.jpg){fig-align="center"}

---

The problem: Big enough to track (40 days), but disappears behind the sun for a period of time.

![Source: *[DALL-E2](https://openai.com/product/dall-e-2) image. "Ceres the asteroid moving between the earth and the sun."*](assets/ceres_sun.jpeg){fig-align="center"}

---

![](assets/gauss.jpg){fig-align="center"}

- Carl Friedrich Gauss was able to predict the location of Ceres by developing a technique of least squares approximation using just 3 observations (out of 22).
- The *most accurate* prediction of Ceres' location at the time.

---

![](assets/legendre.jpg){fig-align="center"}

- The first published evidence of least squares was in 1809, however, by Adrien-Marie Legendre

---

![](assets/galton.jpg){fig-align="center"}

- Sir Francis Galton (1822-1911) was the first to apply least squares to the analysis of bivariate relationships: first use of the term "regression" in 1886.

## Example: Galton's data

- Galton's data on the heights of parents and their children
- Is there a relationship between the heights of parents and their children?

```{r}
#| code-fold: true
library(HistData)
data(Galton)
fit <- lm(child ~ parent, data = Galton)
ggplot(Galton, aes(x = parent, y = child)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "firebrick") +
  labs(x = "Parent height (inches)", y = "Child height (inches)")
```

**How did we end up with the line in the plot above?**

# Fitting the model

## How do we fit a line?

- *Minimise* the sum of the squared residuals:

$$\color{firebrick}{argmin_{\beta_0, \beta_1}} \sum_{i=1}^n (y_i - \color{royalblue}{(\beta_0 + \beta_1 x_i)})^2$$ 


![[Source](https://github.com/Enchufa2/ls-springs)](assets/leastsquares.gif){fig-align="center"}

## Residuals, $\hat \epsilon$

$$ \color{firebrick}{\hat{\epsilon_i}} = \color{royalblue}{y_i} - \color{forestgreen}{\hat{y_i}} $$

. . .

```{r}
#| code-fold: true
# simulate example data
set.seed(340)
x <- runif(8, 0, 30)
y <- 5*x + rnorm(8, 0, 40)
df <- data.frame(x, y)

# fit linear model, add residual vertical lines as arrows
mod <- lm(y ~ x, data = df)
p1 <- ggplot(df, aes(x, y)) +
  geom_point() +
  geom_segment(aes(xend = x, yend = fitted(mod)), 
               arrow = arrow(length = unit(0.2, "cm")), 
               color = "royalblue") +
  labs(x = "x", y = "y") 

p1 + 
  geom_smooth(method = "lm", se = FALSE, color = "firebrick") +
  annotate("text", x = 6.3, y = -6, size = 7, 
    label = expression(hat(epsilon[i])), colour = "royalblue") +
  annotate("text", x = 5.6, y = 25, size = 7, 
    label = expression(hat(y[i])), colour = "forestgreen") +
  annotate("text", x = 5.6, y = -36, size = 7, 
    label = expression(y[i]), colour = "firebrick")
```

## Slope, $\beta_1$

<!-- $$ Y_i = \beta_0 + \beta_1 x_i + \epsilon_i $$ -->

$$ \beta_1 = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2} =  \frac{Cov(x,y)}{Var(x)} = \frac{SS_{xy}}{SS_{xx}} $$



```{r}
#| code-fold: true
# Calculate slope from df
beta1 <- sum((df$x - mean(df$x)) * (df$y - mean(df$y))) / 
  sum((df$x - mean(df$x))^2)
# beta0 <- mean(df$y) - beta1 * mean(df$x)

p1 +   
  geom_smooth(method = "lm", se = FALSE, color = "firebrick", linetype = 2) +
  # label the line
  annotate("text", x = 15, y = 65, size = 7, 
    label = expression(beta[1]), colour = "firebrick")

```


<!-- $$ \beta_0 = \bar{y} - \beta_1 \bar{x} $$ -->

## Intercept

$$ \beta_0 = \bar{y} - \beta_1 \bar{x} $$


```{r}
#| code-fold: true
# calculate mean y from df
ybar <- mean(df$y)
xbar <- mean(df$x)
beta0 <- ybar - beta1 * xbar

p1 + geom_vline(xintercept = xbar, linetype = "dashed", color = "slateblue") +
  geom_hline(yintercept = ybar, linetype = "dashed", color = "slateblue") +
  # label the lines
  annotate("text", x = 25, y = ybar*0.8, size = 7, 
    label = expression(bar(y)), colour = "slateblue") +
  annotate("text", x = xbar*1.05, y = 150, size = 7, 
    label = expression(bar(x)), colour = "slateblue") +
  geom_smooth(method = "lm", se = FALSE, color = "firebrick", linetype = 2) +
  # extend the geom_smooth line to intercept x=0
  geom_segment(aes(x = xbar, y = ybar, xend = 0, yend = beta0), 
               color = "firebrick", linetype = 2) +
  # label the slope line
  annotate("text", x = 15, y = 65, size = 7, 
    label = expression(beta[1]), colour = "firebrick") +
  # add a dot at the intercept
  geom_point(aes(x = 0, y = beta0), color = "seagreen", size = 3) +
  # label the intercept
  annotate("text", x = 0, y = beta0 *1.4, size = 7, 
    label = expression(beta[0]), colour = "seagreen")
  
```


# It's a lot easier in R...

## Fitting a linear model in R

Is there a relationship between the heights of parents and their children?

```{r}
fit <- lm(child ~ parent, data = Galton)
fit
```

```{r}
# remotes::install_github("datalorax/equatiomatic")
equatiomatic::extract_eq(fit, use_coefs = TRUE, coef_digits = 3)
```

. . .

But is the model any good?

# Assessing model fit

## Assumptions

The data **must** meet certain criteria, which we often call *assumptions*. They can be remembered using **LINE**:

- **L**inearity. The relationship between $y$ and $x$ is linear.
- **I**ndependence. The errors $\epsilon$ are independent.
- **N**ormal. The errors $\epsilon$ are normally distributed.
- **E**qual Variance. At each value of $x$, the variance of $y$ is the same i.e. homoskedasticity, or constant variance.

. . .

:::{.callout-tip}
All but the independence assumption can be assessed using diagnostic plots. 
:::

## Assumptions: Why do we care?

- If the assumptions are met, then we can be confident that the model is a good representation of the data.
- If they are *not* met, the results are still presented, but our interpretation of the model is likely to be flawed.

. . .

:::{.callout-warning}
R will not warn you if the assumptions are not met. It is up to you to check them!
:::

## How do we check the assumptions?

Recall that the linear model is a **deterministic straight line equation** $y = c + mx$ plus some **random noise** $\epsilon$:

$$ Y_i = \beta_0 + \beta_1 x + \epsilon $$

- **If the only source of variation in $y$ is $\epsilon$, then we can check our assumptions by just looking at the residuals $\hat{\epsilon}$.**

### How do we get the residuals?

- Fit the model...
- Residuals need to be calculated from the model, not from the raw data.
- In R, these values are stored automatically.


## Another way to look at residuals

![](assets/residual.jpg){fig-align="center"}

Once you have fitted the line, it does not change. The residuals are the vertical distances between the points (not shown) and the line.

# Checking assumptions

## 1-step

```{r}
par(mfrow = c(2,2)) # need to do this to get 4 plots on one page
plot(fit)
```

---

- Residuals vs fitted: check for linearity, equal variance.
- Q-Q residuals: check for normality.
- Scale-location: check for equal variance (standardised).
- Residuals vs leverage: check for outliers (influential points).

```{r}
#| echo: false
par(mfrow = c(2,2)) # need to do this to get 4 plots on one page
plot(fit)
```

## Assumption: Linearity

- Residuals vs. fitted plot looks at the relationship between the residuals and the fitted values.
- If the relationship is linear:
  - Residuals should be randomly scattered around the horizontal axis.
  - The red line should be reasonably straight.

```{r}
plot(fit, which = 1)
```

## Assumption: Normality

- Q-Q plot looks at the distribution of the residuals, like a histogram, but "cleaner".
- Sometimes, a histogram is still useful to see the shape of the distribution.

```{r}
par(mfrow = c(1,2))
plot(fit, which = 2)
hist(rstandard(fit))
```

## Assumption: Normality

- If normally distributed, the points should follow the red line.
- Deviation from the red line is common in the tails, but not in the middle.

. . .

### Tips

- **Light-tailed**: small variance in residuals, resulting in a narrow distribution
- **Heavy-tailed**: many extreme positive and negative residuals, resulting in a wide distribution
- **Left-skewed** (n shape): more data falls to the left of the mean
- **Right-skewed** (u shape): more data falls to the right of the mean

---

```{r}
#| code-fold: true
set.seed(915)
x <- rnorm(100)
y <- 2 + 5 * x + rchisq(100, df = 2)
df <- data.frame(x, y)
fit_eg <- lm(y ~ x, data = df)
par(mfrow = c(1,2))
plot(fit_eg, which = 2)
hist(rstandard(fit_eg))
```

---

```{r}
#| code-fold: true
set.seed(1028)
x <- rnorm(100)
y <- 2 + 5 * x + rchisq(100, df = 3) * -1
df <- data.frame(x, y)
fit_eg <- lm(y ~ x, data = df)
par(mfrow = c(1,2))
plot(fit_eg, which = 2)
hist(rstandard(fit_eg))
```

---

```{r}
#| code-fold: true
set.seed(1028)
x <- rnorm(100)
y <- 2 + 5 * x + rnbinom(100, 10, .5)
df <- data.frame(x, y)
fit_eg <- lm(y ~ x, data = df)
par(mfrow = c(1,2))
plot(fit_eg, which = 2)
hist(rstandard(fit_eg))
```

## Assumption: Equal variances

- If variances are equal, the points should be randomly scattered around the horizontal axis.
- The red line should be more or less horizontal.

```{r}
plot(fit, which = 3)
```

## Assumption: Equal variances

- If variances are not equal we *may* see:
  - A funnel shape, where the points are more spread out at the ends than in the middle. Sometimes also called "fanning".
  - Patterns in the scale-location plot, such as a curve or a wave, indicating that the variance is changing.
- Look at the red line for a general trend, but don't depend on it too much.


```{r}
#| code-fold: true
set.seed(915)
x <- rnorm(100)
y <- 2 + 5 * x^2 + rchisq(100, df = 2)
df <- data.frame(x, y)
fit_eg <- lm(y ~ x, data = df)
plot(fit_eg, which = 3)
```

## Outliers

- **Leverage** is a measure of how far away the predictor variable is from the mean of the predictor variable.
- The Residuals vs Leverage plot shows the relationship between the residuals and the leverage of each point.
- **Cook's distance** is a measure of how much the model would change if a point was removed.

---

- In general, points with **high leverage** and **high Cook's distance** are considered outliers.

```{r}
plot(fit, which = 5)
```

--- 

```{r}
#| code-fold: true
set.seed(1028)
x <- rnorm(100)
y <- 2 + 5 * x + rnbinom(100, 10, .5)
y[60] <- y[60] + 30
df <- data.frame(x, y)
# Add an outlier
fit_eg <- lm(y ~ x, data = df)
plot(fit_eg, which = 5)
```

We don't want points to exceed the dashed line (which appears once they approach the Cook's distance), because that means they are likely to influence the model greatly.

# Handling violations

## What can we do?

- Depends on the violation and the type of data i.e. circumstances.
  - If data is **non-linear**, try a transformation of the response variable $y$, from light to extreme:
    - root: $\sqrt{y}$ or $\sqrt{y+1}$ if $y$ contains zeros
    - log: $\log(y)$ or $\log(y+1)$ if $y$ contains zeros
    - inverse: $\frac{1}{y}$ or $\frac{1}{y+1}$ if $y$ contains zeros

. . .

  - If data is **not normally distributed**, try a transformation of the response variable $y$ first, otherwise transform the predictor variable $x$. Both can be done at the same time.

. . .

  - If **equal variances** assumption is violated, same as above.
  - If **outliers** are present, try removing them, or transforming the response variable $y$.

## What if that doesn't work?

- If the assumptions are still violated after trying the above, you can try:
  - Using a different model e.g. generalized linear model.
  - Using a different type of regression e.g. logistic regression.
  - Using a non-parametric test.

### Model assumptions validated. Now what?

# Inference
What can we say about the model based on our data?

> What can we understand about the relationship between `child` and `parent`?

# See you tomorrow

**Questions? Comments?**

Slides made with [Quarto](https://quarto.org)
